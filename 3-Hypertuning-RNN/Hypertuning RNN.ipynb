{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from mltrainer import rnn_models, Trainer\n",
    "from torch import optim\n",
    "\n",
    "from mads_datasets import datatools\n",
    "import mltrainer\n",
    "mltrainer.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Iterators\n",
    "We will be using an interesting dataset. [link](https://tev.fbk.eu/resources/smartwatch)\n",
    "\n",
    "From the site:\n",
    "> The SmartWatch Gestures Dataset has been collected to evaluate several gesture recognition algorithms for interacting with mobile applications using arm gestures. Eight different users performed twenty repetitions of twenty different gestures, for a total of 3200 sequences. Each sequence contains acceleration data from the 3-axis accelerometer of a first generation Sony SmartWatchâ„¢, as well as timestamps from the different clock sources available on an Android device. The smartwatch was worn on the user's right wrist. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 11:37:18.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\denni\\.cache\\mads_datasets\\gestures\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 2600/2600 [00:00<00:00, 4995.78it/s]\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 651/651 [00:00<00:00, 5081.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import PaddedPreprocessor\n",
    "preprocessor = PaddedPreprocessor()\n",
    "\n",
    "gesturesdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.GESTURES)\n",
    "streamers = gesturesdatasetfactory.create_datastreamer(batchsize=32, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30, 3]),\n",
       " tensor([18, 10, 17, 10,  8, 15, 17,  0,  0,  0,  2,  1, 14, 16, 10,  2,  4, 11,\n",
       "         16, 15,  9,  7,  8, 18, 18, 16, 12,  7, 18,  1, 11,  5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the shape?\n",
    "What does it mean that the shapes are sometimes (32, 27, 3), but a second time might look like (32, 30, 3)? In other words, the second (or first, if you insist on starting at 0) dimension changes. Why is that? How does the model handle this? Do you think this is already padded, or still has to be padded?\n",
    "\n",
    "\n",
    "# 2 Excercises\n",
    "Lets test a basemodel, and try to improve upon that.\n",
    "\n",
    "Fill the gestures.gin file with relevant settings for `input_size`, `hidden_size`, `num_layers` and `horizon` (which, in our case, will be the number of classes...)\n",
    "\n",
    "As a rule of thumbs: start lower than you expect to need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import TrainerSettings, ReportTypes\n",
    "from mltrainer.metrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn_models.BaseRNN(\n",
    "    input_size=3,\n",
    "    hidden_size=64,\n",
    "    num_layers=1,\n",
    "    horizon=20,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model. What is the output shape you need? Remember, we are doing classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y, yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the accuracy? What would you expect from blind guessing?\n",
    "\n",
    "Check shape of `y` and `yhat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at the output of yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0205, -0.0310,  0.0290,  0.1616, -0.0082,  0.0675, -0.0680, -0.0803,\n",
       "         0.0125,  0.0340, -0.0595,  0.0148,  0.1036, -0.0333,  0.1702, -0.0325,\n",
       "        -0.0459,  0.1505,  0.0730, -0.1206], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense to you? If you are unclear, go back to the classification problem with the MNIST, where we had 10 classes.\n",
    "\n",
    "We have a classification problem, so we need Cross Entropy Loss.\n",
    "Remember, [this has a softmax built in](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9868, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(yhat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# on my mac, at least for the BaseRNN model, mps does not speed up training\n",
    "# probably because the overhead of copying the data to the GPU is too high\n",
    "# so i override the device to cpu\n",
    "device = \"cpu\"\n",
    "# however, it might speed up training for larger models, with more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the settings for the trainer and the different types of logging you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 50\n",
       "metrics: [Accuracy]\n",
       "logdir: gestures\n",
       "train_steps: 81\n",
       "valid_steps: 20\n",
       "reporttypes: [<ReportTypes.TOML: 'TOML'>, <ReportTypes.TENSORBOARD: 'TENSORBOARD'>, <ReportTypes.MLFLOW: 'MLFLOW'>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 5, 'delta': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = TrainerSettings(\n",
    "    epochs=50, # increase this to about 100 for training\n",
    "    metrics=[accuracy],\n",
    "    logdir=Path(\"gestures\"),\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TOML, ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    "    earlystop_kwargs = {\n",
    "        \"save\": False, # save every best model, and restore the best one\n",
    "        \"verbose\": True,\n",
    "        \"patience\": 5, # number of epochs with no improvement after which training will be stopped\n",
    "        \"delta\": 0.0, # minimum change to be considered an improvement\n",
    "    }\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    output_size: int\n",
    "    dropout: float = 0.0\n",
    "\n",
    "class GRUmodel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.input_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True,\n",
    "            num_layers=config.num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(config.hidden_size, config.output_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, _ = self.rnn(x)\n",
    "        last_step = x[:, -1, :]\n",
    "        yhat = self.linear(last_step)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM variant: swap GRU for LSTM and use the last hidden state\n",
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=config.input_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True,\n",
    "            num_layers=config.num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(config.hidden_size, config.output_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out, (h_n, c_n) = self.rnn(x)\n",
    "        # h_n shape: (num_layers, batch, hidden_size) -> take last layer\n",
    "        last = h_n[-1]\n",
    "        yhat = self.linear(last)\n",
    "        return yhat\n",
    "\n",
    "# Quick instantiation example (use later when training):\n",
    "# config_lstm = ModelConfig(input_size=3, hidden_size=128, num_layers=2, output_size=20, dropout=0.5)\n",
    "# model = LSTMmodel(config_lstm)\n",
    "# yhat = model(x); yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 13:11:30.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to gestures\\20260107-131130\u001b[0m\n",
      "\u001b[32m2026-01-07 13:11:30.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 44.09it/s]\n",
      "\u001b[32m2026-01-07 13:11:32.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 2.5014 test 2.3797 metric ['0.2016']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 38.89it/s]\n",
      "\u001b[32m2026-01-07 13:11:34.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 2.0589 test 2.0007 metric ['0.3047']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.12it/s]\n",
      "\u001b[32m2026-01-07 13:11:36.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 1.4716 test 1.3311 metric ['0.4562']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 42.98it/s]\n",
      "\u001b[32m2026-01-07 13:11:38.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 1.1943 test 1.1116 metric ['0.5344']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 47.02it/s]\n",
      "\u001b[32m2026-01-07 13:11:40.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.9170 test 1.1493 metric ['0.5687']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:11:40.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.1116, current loss 1.1493.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.76it/s]\n",
      "\u001b[32m2026-01-07 13:11:42.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.7166 test 0.7328 metric ['0.7188']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 41.21it/s]\n",
      "\u001b[32m2026-01-07 13:11:44.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.5075 test 0.4620 metric ['0.8562']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.81it/s]\n",
      "\u001b[32m2026-01-07 13:11:46.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.3434 test 0.3038 metric ['0.8859']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.41it/s]\n",
      "\u001b[32m2026-01-07 13:11:48.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.2137 test 0.2497 metric ['0.9281']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.63it/s]\n",
      "\u001b[32m2026-01-07 13:11:50.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.2049 test 0.1696 metric ['0.9531']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 44.43it/s]\n",
      "\u001b[32m2026-01-07 13:11:52.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 10 train 0.1036 test 0.1640 metric ['0.9563']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 38.59it/s]\n",
      "\u001b[32m2026-01-07 13:11:54.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 11 train 0.0970 test 0.0820 metric ['0.9844']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.66it/s]\n",
      "\u001b[32m2026-01-07 13:11:56.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 12 train 0.0699 test 0.0666 metric ['0.9828']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 44.62it/s]\n",
      "\u001b[32m2026-01-07 13:11:58.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 13 train 0.0508 test 0.1209 metric ['0.9719']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:11:58.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0666, current loss 0.1209.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 46.04it/s]\n",
      "\u001b[32m2026-01-07 13:12:00.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 14 train 0.0367 test 0.0822 metric ['0.9844']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:00.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0666, current loss 0.0822.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 46.62it/s]\n",
      "\u001b[32m2026-01-07 13:12:02.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 15 train 0.0252 test 0.0547 metric ['0.9922']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 47.15it/s]\n",
      "\u001b[32m2026-01-07 13:12:04.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 16 train 0.0291 test 0.1407 metric ['0.9625']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:04.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0547, current loss 0.1407.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 44.63it/s]\n",
      "\u001b[32m2026-01-07 13:12:06.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 17 train 0.0273 test 0.0593 metric ['0.9891']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:06.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0547, current loss 0.0593.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 46.92it/s]\n",
      "\u001b[32m2026-01-07 13:12:08.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 18 train 0.0189 test 0.0639 metric ['0.9906']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:08.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0547, current loss 0.0639.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.52it/s]\n",
      "\u001b[32m2026-01-07 13:12:10.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 19 train 0.0282 test 0.0618 metric ['0.9891']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:10.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0547, current loss 0.0618.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 46.75it/s]\n",
      "\u001b[32m2026-01-07 13:12:11.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 20 train 0.0720 test 0.1597 metric ['0.9469']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:11.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0547, current loss 0.1597.Counter 5/5.\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:11.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2026-01-07 13:12:11.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mearly_stopping_save was false, using latest model.Set to true to retrieve best model.\u001b[0m\n",
      " 40%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆ      \u001b[0m| 20/50 [00:41<01:02,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run mercurial-sloth-140 at: http://127.0.0.1:5000/#/experiments/2/runs/3cf258aeca5f4704904aa1f4bd9c9a04\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Training snippet for LSTMmodel (mlflow + Trainer)\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"gestures\")\n",
    "modeldir = Path(\"gestures\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir(parents=True)\n",
    "\n",
    "# configure LSTM and train\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"model\", \"256-hidden-LSTM 2-layer 0.3-dropout\")\n",
    "    mlflow.set_tag(\"dev\", \"Dennis\")\n",
    "    config_lstm = ModelConfig(\n",
    "        input_size=3,\n",
    "        hidden_size=256,\n",
    "        num_layers=2,\n",
    "        output_size=20,\n",
    "        dropout=0.3,\n",
    "    )\n",
    "\n",
    "    model = LSTMmodel(config_lstm)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        settings=settings,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim.Adam,\n",
    "        traindataloader=trainstreamer,\n",
    "        validdataloader=validstreamer,\n",
    "        scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer.loop()\n",
    "\n",
    "    if not settings.earlystop_kwargs[\"save\"]:\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M-\")\n",
    "        modelpath = modeldir / (tag + \"lstm-model.pt\")\n",
    "        torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 13:39:19.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to gestures\\20260107-133919\u001b[0m\n",
      "\u001b[32m2026-01-07 13:39:19.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 26.38it/s]\n",
      "\u001b[32m2026-01-07 13:39:22.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 2.5944 test 2.4013 metric ['0.1984']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.63it/s]\n",
      "\u001b[32m2026-01-07 13:39:25.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 1.7826 test 2.0090 metric ['0.3047']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 26.93it/s]\n",
      "\u001b[32m2026-01-07 13:39:29.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 1.0084 test 0.8641 metric ['0.6281']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.79it/s]\n",
      "\u001b[32m2026-01-07 13:39:31.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.6364 test 0.4904 metric ['0.8375']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.96it/s]\n",
      "\u001b[32m2026-01-07 13:39:34.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3031 test 0.2179 metric ['0.9391']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 27.14it/s]\n",
      "\u001b[32m2026-01-07 13:39:38.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.1364 test 0.1549 metric ['0.9547']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.57it/s]\n",
      "\u001b[32m2026-01-07 13:39:40.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.0944 test 0.1205 metric ['0.9641']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.15it/s]\n",
      "\u001b[32m2026-01-07 13:39:43.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.0572 test 0.1201 metric ['0.9641']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.56it/s]\n",
      "\u001b[32m2026-01-07 13:39:46.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.0442 test 0.0690 metric ['0.9844']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.22it/s]\n",
      "\u001b[32m2026-01-07 13:39:49.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.0463 test 0.0845 metric ['0.9812']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:39:49.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0690, current loss 0.0845.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.15it/s]\n",
      "\u001b[32m2026-01-07 13:39:52.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 10 train 0.0324 test 0.0710 metric ['0.9828']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:39:52.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0690, current loss 0.0710.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 27.86it/s]\n",
      "\u001b[32m2026-01-07 13:39:55.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 11 train 0.0419 test 0.0701 metric ['0.9844']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:39:55.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0690, current loss 0.0701.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.62it/s]\n",
      "\u001b[32m2026-01-07 13:39:58.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 12 train 0.0238 test 0.0600 metric ['0.9906']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.44it/s]\n",
      "\u001b[32m2026-01-07 13:40:01.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 13 train 0.0165 test 0.0486 metric ['0.9891']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.42it/s]\n",
      "\u001b[32m2026-01-07 13:40:04.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 14 train 0.0068 test 0.0467 metric ['0.9891']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 28.56it/s]\n",
      "\u001b[32m2026-01-07 13:40:07.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 15 train 0.0060 test 0.0500 metric ['0.9875']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:07.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0467, current loss 0.0500.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 20.57it/s]\n",
      "\u001b[32m2026-01-07 13:40:11.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 16 train 0.0040 test 0.0631 metric ['0.9891']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:11.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0467, current loss 0.0631.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 22.40it/s]\n",
      "\u001b[32m2026-01-07 13:40:15.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 17 train 0.0023 test 0.0327 metric ['0.9938']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 27.50it/s]\n",
      "\u001b[32m2026-01-07 13:40:18.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 18 train 0.0107 test 0.0458 metric ['0.9891']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:18.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0327, current loss 0.0458.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 27.07it/s]\n",
      "\u001b[32m2026-01-07 13:40:22.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 19 train 0.0039 test 0.0415 metric ['0.9922']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:22.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0327, current loss 0.0415.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 25.38it/s]\n",
      "\u001b[32m2026-01-07 13:40:25.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 20 train 0.0306 test 0.1268 metric ['0.9672']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:25.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0327, current loss 0.1268.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 21.11it/s]\n",
      "\u001b[32m2026-01-07 13:40:29.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 21 train 0.0331 test 0.0821 metric ['0.9781']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:29.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0327, current loss 0.0821.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 24.32it/s]\n",
      "\u001b[32m2026-01-07 13:40:33.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 22 train 0.0203 test 0.0883 metric ['0.9750']\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:33.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0327, current loss 0.0883.Counter 5/5.\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:33.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2026-01-07 13:40:33.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mearly_stopping_save was false, using latest model.Set to true to retrieve best model.\u001b[0m\n",
      " 44%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–     \u001b[0m| 22/50 [01:13<01:33,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bustling-calf-417 at: http://127.0.0.1:5000/#/experiments/2/runs/00b8e25db2854afa8fa5f5707dcaa889\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Conv1D + GRU variant: apply temporal convolution before the RNN (PyTorch expects Conv1d input as (B, C, T))\n",
    "class ConvGRUmodel(nn.Module):\n",
    "    def __init__(self, config, conv_channels=32, kernel_size=5) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conv = nn.Conv1d(in_channels=config.input_size, out_channels=conv_channels, kernel_size=kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.bn = nn.BatchNorm1d(conv_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = nn.GRU(input_size=conv_channels, hidden_size=config.hidden_size, num_layers=config.num_layers, batch_first=True, dropout=config.dropout)\n",
    "        self.linear = nn.Linear(config.hidden_size, config.output_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: (batch, timesteps, channels) -> permute to (batch, channels, timesteps) for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        # back to (batch, timesteps, channels=conv_channels)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, h = self.rnn(x)\n",
    "        last = h[-1]\n",
    "        yhat = self.linear(last)\n",
    "        return yhat\n",
    "\n",
    "# Training snippet for ConvGRU (use the best-performing hyperparams as baseline)\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"gestures\")\n",
    "modeldir = Path(\"gestures\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir(parents=True)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"model\", \"ConvGRU-32conv-256hid-2layer-0.0dropout-5-kernel\")\n",
    "    mlflow.set_tag(\"dev\", \"Dennis\")\n",
    "    config_conv = ModelConfig(\n",
    "        input_size=3,\n",
    "        hidden_size=256,\n",
    "        num_layers=2,\n",
    "        output_size=20,\n",
    "        dropout=0.0,\n",
    "    )\n",
    "\n",
    "    model = ConvGRUmodel(config_conv, conv_channels=32, kernel_size=5)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        settings=settings,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim.Adam,\n",
    "        traindataloader=trainstreamer,\n",
    "        validdataloader=validstreamer,\n",
    "        scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer.loop()\n",
    "\n",
    "    if not settings.earlystop_kwargs[\"save\"]:\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M-\")\n",
    "        modelpath = modeldir / (tag + \"convgru-model.pt\")\n",
    "        torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    input_size=3,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    output_size=20,\n",
    "    dropout=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-07 12:16:11.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to gestures\\20260107-121611\u001b[0m\n",
      "\u001b[32m2026-01-07 12:16:11.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:03<00:00, 25.58it/s]\n",
      "\u001b[32m2026-01-07 12:16:14.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 2.4408 test 2.1745 metric ['0.2953']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.71it/s]\n",
      "\u001b[32m2026-01-07 12:16:17.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 1.5785 test 1.4091 metric ['0.4781']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.96it/s]\n",
      "\u001b[32m2026-01-07 12:16:20.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.7509 test 0.5085 metric ['0.8359']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.80it/s]\n",
      "\u001b[32m2026-01-07 12:16:23.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3656 test 0.2600 metric ['0.9359']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.81it/s]\n",
      "\u001b[32m2026-01-07 12:16:26.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.1171 test 0.0899 metric ['0.9797']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.01it/s]\n",
      "\u001b[32m2026-01-07 12:16:29.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.0801 test 0.2051 metric ['0.9406']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:16:29.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0899, current loss 0.2051.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.61it/s]\n",
      "\u001b[32m2026-01-07 12:16:32.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2020 test 0.1990 metric ['0.9531']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:16:32.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0899, current loss 0.1990.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.68it/s]\n",
      "\u001b[32m2026-01-07 12:16:35.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.0657 test 0.0687 metric ['0.9828']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 27.71it/s]\n",
      "\u001b[32m2026-01-07 12:16:38.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 8 train 0.0338 test 0.0610 metric ['0.9812']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.43it/s]\n",
      "\u001b[32m2026-01-07 12:16:41.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 9 train 0.0176 test 0.0786 metric ['0.9781']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:16:41.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0610, current loss 0.0786.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.78it/s]\n",
      "\u001b[32m2026-01-07 12:16:44.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 10 train 0.0078 test 0.0474 metric ['0.9891']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.35it/s]\n",
      "\u001b[32m2026-01-07 12:16:47.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 11 train 0.0036 test 0.0448 metric ['0.9922']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 28.51it/s]\n",
      "\u001b[32m2026-01-07 12:16:50.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 12 train 0.0025 test 0.0444 metric ['0.9891']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.12it/s]\n",
      "\u001b[32m2026-01-07 12:16:53.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 13 train 0.0017 test 0.0400 metric ['0.9906']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.67it/s]\n",
      "\u001b[32m2026-01-07 12:16:56.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 14 train 0.0015 test 0.0485 metric ['0.9891']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:16:56.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0400, current loss 0.0485.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.98it/s]\n",
      "\u001b[32m2026-01-07 12:16:59.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 15 train 0.0012 test 0.0504 metric ['0.9891']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:16:59.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0400, current loss 0.0504.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 29.40it/s]\n",
      "\u001b[32m2026-01-07 12:17:02.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 16 train 0.0011 test 0.0461 metric ['0.9906']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:17:02.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0400, current loss 0.0461.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 31.13it/s]\n",
      "\u001b[32m2026-01-07 12:17:05.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 17 train 0.0043 test 0.0521 metric ['0.9922']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:17:05.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0400, current loss 0.0521.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:02<00:00, 30.64it/s]\n",
      "\u001b[32m2026-01-07 12:17:07.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 18 train 0.0523 test 0.1585 metric ['0.9578']\u001b[0m\n",
      "\u001b[32m2026-01-07 12:17:07.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.0400, current loss 0.1585.Counter 5/5.\u001b[0m\n",
      "\u001b[32m2026-01-07 12:17:07.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2026-01-07 12:17:07.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mearly_stopping_save was false, using latest model.Set to true to retrieve best model.\u001b[0m\n",
      " 36%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–Œ      \u001b[0m| 18/50 [00:56<01:40,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run abundant-loon-913 at: http://127.0.0.1:5000/#/experiments/2/runs/8e42aaca0d2e474c8e55c8ea80ad0087\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"gestures\")\n",
    "modeldir = Path(\"gestures\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir(parents=True)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"model\", \"512-hidden-GRU 1-layer 0.3-dropout\")\n",
    "    mlflow.set_tag(\"dev\", \"Dennis\")\n",
    "    config = ModelConfig(\n",
    "        input_size=3,\n",
    "        hidden_size=512,\n",
    "        num_layers=1,\n",
    "        output_size=20,\n",
    "        dropout=0.3,\n",
    "    )\n",
    "\n",
    "    model = GRUmodel(\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        settings=settings,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim.Adam,\n",
    "        traindataloader=trainstreamer,\n",
    "        validdataloader=validstreamer,\n",
    "        scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer.loop()\n",
    "\n",
    "    if not settings.earlystop_kwargs[\"save\"]:\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M-\")\n",
    "        modelpath = modeldir / (tag + \"model.pt\")\n",
    "        torch.save(model, modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to update the code above by changing the hyperparameters.\n",
    "    \n",
    "To discern between the changes, also modify the tag mlflow.set_tag(\"model\", \"new-tag-here\") where you add\n",
    "a new tag of your choice. This way you can keep the models apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 81/81 [00:01<00:00, 45.23it/s]\n",
      "\u001b[32m2026-01-07 11:57:16.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m175\u001b[0m - \u001b[1mResuming epochs from previous training at 63\u001b[0m\n",
      "\u001b[32m2026-01-07 11:57:17.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 63 train 0.0600 test 0.1814 metric ['0.9578']\u001b[0m\n",
      "\u001b[32m2026-01-07 11:57:17.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.1803, current loss 0.1814.Counter 6/5.\u001b[0m\n",
      "\u001b[32m2026-01-07 11:57:17.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2026-01-07 11:57:17.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mearly_stopping_save was false, using latest model.Set to true to retrieve best model.\u001b[0m\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/50 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.loop() # if you want to pick up training, loop will continue from the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercises:\n",
    "\n",
    "- try to improve the RNN model\n",
    "- test different things. What works? What does not?\n",
    "- experiment with either GRU or LSTM layers, create your own models. Have a look at `mltrainer.rnn_models` for inspiration. \n",
    "- experiment with adding Conv1D layers. Think about the necessary input-output dimensions of your tensors before and after each layer.\n",
    "\n",
    "You should be able to get above 90% accuracy with the dataset.\n",
    "Create a report of 1 a4 about your experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
